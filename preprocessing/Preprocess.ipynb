{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "## Version as of 2021-1-1\n",
    "\n",
    "#csv_dir = '/home/ghhur/data/input' # '../ghhur_data_input/' in old system\n",
    "#input_dir = '/home/ghhur/data/cohort'\n",
    "#output_dir = '/home/ghhur/data/1_processed'\n",
    "\n",
    "\n",
    "csv_dir = '/home/ghhur/data/csv' # '../ghhur_data_input/' in old system\n",
    "input_dir = '/home/ghhur/data/input'\n",
    "output_dir = '/home/ghhur/data/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirmake(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    elif os.path.isdir(path):\n",
    "        print('path {} already exist!'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path /home/ghhur/data/csv already exist!\n",
      "path /home/ghhur/data/csv/mimic already exist!\n",
      "path /home/ghhur/data/csv/eicu already exist!\n",
      "cohort file is opened!\n"
     ]
    }
   ],
   "source": [
    "#csv_dir check\n",
    "dirmake(csv_dir)\n",
    "dirmake(os.path.join(csv_dir,'mimic'))\n",
    "dirmake(os.path.join(csv_dir,'eicu'))\n",
    "mimic_check = os.path.isfile(os.path.join(csv_dir, 'mimic_cohort_dx_done.pk'))\n",
    "eicu_check = os.path.isfile(os.path.join(csv_dir, 'eicu_cohort_dx_done.pk'))\n",
    "if mimic_check and eicu_check:\n",
    "    print('cohort file is opened!')\n",
    "    \n",
    "else:\n",
    "    print('cohort file location is wrong, please put mimic & eicu cohort file on csv directory')\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory is...  \"/home/ghhur/preprocess_code\"\n",
      "Making output directory finish!\n",
      "path ./input already exist!\n",
      "path ./input/all already exist!\n",
      "path ./input/all/embed_vocab_file already exist!\n",
      "Making input directory finish!\n"
     ]
    }
   ],
   "source": [
    "print('current working directory is...  \"{}\"'.format(os.getcwd()))\n",
    "#output dir generate\n",
    "task_list = ['readmission', 'mortality', 'los_3days', 'los_7days', 'dx_dpeth1_unique']\n",
    "model_type_list = ['singleRNN', 'cls_learnable']\n",
    "source_file_list = ['mimic', 'eicu', 'both']\n",
    "dirmake(output_dir)\n",
    "dirmake(os.path.join(output_dir, 'all'))\n",
    "for model_type in model_type_list:\n",
    "    tmp_dir_model = os.path.join(output_dir, 'all', model_type)\n",
    "    dirmake(tmp_dir_model)\n",
    "    for source_file in source_file_list:\n",
    "            tmp_dir_source_file = os.path.join(tmp_dir_model,source_file)\n",
    "            dirmake(tmp_dir_source_file)\n",
    "            for task in task_list:\n",
    "                tmp_dir_task = os.path.join(tmp_dir_source_file, task)\n",
    "                dirmake(tmp_dir_task)\n",
    "print('Making output directory finish!')\n",
    "#input dir generate\n",
    "input_dir = './input'\n",
    "dirmake(input_dir)\n",
    "dirmake(os.path.join(input_dir, 'all'))\n",
    "dirmake(os.path.join(input_dir, 'all', 'embed_vocab_file'))\n",
    "print('Making input directory finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['mimic','eicu']\n",
    "items= ['lab','med','inf'] # items = ['dx','lab','med','trt','chart','inf']\n",
    "\n",
    "mimic_csv_files = {'lab':['LABEVENTS'], 'med':['PRESCRIPTIONS'],  # mimic dictionary\n",
    "                        'inf': ['INPUTEVENTS_CV', 'INPUTEVENTS_MV']} \n",
    "eicu_csv_files = {'lab':['lab'], 'med':['medication'],'inf':['infusionDrug']}\n",
    "                  # eicu dictionary\n",
    "                 \n",
    "\n",
    "mimic_dictionary_file = {'LABEVENTS':'D_LABITEMS', \n",
    "                         'INPUTEVENTS_CV':'D_ITEMS', 'INPUTEVENTS_MV':'D_ITEMS'}\n",
    "#eicu chartevent 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_columns_map = {'LABEVENTS':\n",
    "                         {'HADM_ID':'ID','CHARTTIME':'order_time','ITEMID':'code_name',\n",
    "                          'FLAG':'issue'},\n",
    "                     'PRESCRIPTIONS':\n",
    "                         {'HADM_ID':'ID','STARTDATE':'order_time',\n",
    "                          'DRUG':'code_name', 'ROUTE':'route', 'PROD_STRENGTH':'prod'},                                      \n",
    "                      'INPUTEVENTS_CV': \n",
    "                         {'HADM_ID':'ID','CHARTTIME':'order_time', \n",
    "                          'ITEMID':'code_name','AMOUNTUOM':'value_uom', 'RATEUOM':'rateuom',\n",
    "                          'STOPPED':'issue'},\n",
    "                      'INPUTEVENTS_MV': \n",
    "                         {'HADM_ID':'ID', 'STARTTIME':'order_time','AMOUNTUOM':'value_uom', 'RATEUOM':'rateuom',\n",
    "                          'ITEMID':'code_name', 'STOPPED':'issue'}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eicu_columns_map =  {'lab':\n",
    "                         {'patientunitstayid':'ID', 'labresultoffset':'order_offset','labname':'code_name'},\n",
    "                     'medication':\n",
    "                         {'patientunitstayid':'ID','drugstartoffset':'order_offset','drugname':'code_name', 'routeadmin':'route',\n",
    "                          'ordercancelled':'issue'},      \n",
    "                      'infusionDrug':\n",
    "                         {'patientunitstayid':'ID','infusionoffset':'order_offset', 'drugname':'code_name'}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_map = {'LABEVENTS': ['abnormal'],                            \n",
    "             'INPUTEVENTS_CV':['Restart','NotStopd'] ,\n",
    "             'INPUTEVENTS_MV': ['Rewritten', 'Changed', 'Paused', 'Flushed', 'Stopped'],\n",
    "             'medication': ['Yes'],          \n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInput : argument -> output : df, cohort\\nprocessing:\\ncolumn_rename-> cohort_filtering -> issue_delete -> if mimic, name_dict -> df, cohort\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Input : argument -> output : df, cohort\n",
    "processing:\n",
    "column_rename-> cohort_filtering -> issue_delete -> if mimic, name_dict -> df, cohort\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_init():\n",
    "    def __init__(self, file:str, src:str, item:str, columns_map):\n",
    "        self.columns_map = columns_map\n",
    "        self.file = file\n",
    "        self.src = src\n",
    "        self.item = item\n",
    "        self.input_folder = os.path.join(csv_dir,src)\n",
    "        df_path = os.path.join(self.input_folder,file+'.csv')\n",
    "        cohort_path = os.path.join(input_dir, src+'_cohort_dx_done.pk')\n",
    "        # Read in cohort pickle and appropriate csv file\n",
    "        self.cohort = pd.read_pickle(cohort_path).reset_index(drop=True)\n",
    "        print('cohort load finish!')\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        print('csv file load finish!')\n",
    "        if self.src == 'mimic':\n",
    "            self.cohort=self.cohort.rename({'HADM_ID':'ID'},axis='columns') # rename ID columns in pickle as needed\n",
    "        elif self.src == 'eicu':\n",
    "             self.cohort=self.cohort.rename({'patientunitstayid':'ID'},axis='columns')\n",
    "        \n",
    "    def column_rename(self, df):      # for similar col names\n",
    "        df = df.rename(self.columns_map[self.file], axis='columns')\n",
    "        return df                          \n",
    "        \n",
    "    def cohort_filtering(self, df):    # take only the observations from the .csv file which are in the cohort\n",
    "        df_id=df['ID']\n",
    "        cohort_id=self.cohort['ID']\n",
    "        df = df[df_id.isin(cohort_id)].reset_index(drop=True)\n",
    "        \n",
    "        cohort = self.cohort[cohort_id.isin(df_id)].reset_index(drop=True) # drop cohort obs which !in(csv)\n",
    "    #    df.replace(' ', '_', regex=True, inplace=True) <-- earlier preprocessing code?\n",
    "       \n",
    "        if 'ICUSTAY_ID' in df.columns: # drop all with missing ICUSTAY_ID (mimic)\n",
    "            df = df.loc[df['ICUSTAY_ID'].isnull()==False].reset_index(drop=True)\n",
    "        if 'start_time' in df.columns: # if null end_time, input start_time (from e.g. mimic prescriptions)\n",
    "            indexes = df[df['end_time'].isnull()==True]['start_time'].index\n",
    "            df['end_time'][indexes] = df[df['end_time'].isnull()==True]['start_time']\n",
    "        df.fillna('null', inplace=True)\n",
    "        return df, cohort\n",
    "    \n",
    "    \n",
    "    def issue_delete(self, df): # e.g. drop if 'order_cancelled'==yes in eicu medications\n",
    "        if 'issue' in df.columns:\n",
    "            issue_label = issue_map[self.file]\n",
    "            df.drop(df[df['issue'].isin(issue_label)].index, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def name_dict(self, df): # for mimic files, create code_name from appropriate dictionary file (key e.g. ICD9)\n",
    "        if self.file in mimic_dictionary_file:\n",
    "            dict_name=mimic_dictionary_file[self.file]\n",
    "            dict_path = os.path.join(self.input_folder, dict_name+'.csv')\n",
    "            code_dict = pd.read_csv(dict_path)\n",
    "            if dict_name in ['D_ICD_DIAGNOSES', 'D_ICD_PROCEDURES']:\n",
    "                key = code_dict['ICD9_CODE']\n",
    "                value = code_dict['LONG_TITLE']\n",
    "            else:\n",
    "                key = code_dict['ITEMID']\n",
    "                value = code_dict['LABEL']\n",
    "            code_dict = dict(zip(key,value))\n",
    "            df['code_name'] = df['code_name'].map(code_dict)\n",
    "        return df          \n",
    "                \n",
    "    def __call__(self):        \n",
    "        print('column_rename start!')\n",
    "        df = self.column_rename(self.df)\n",
    "        print('column_rename finish!')\n",
    "        print('cohort_filtering start!')\n",
    "        df, cohort = self.cohort_filtering(df)\n",
    "        print('cohort_filtering finish!')\n",
    "        print('issue_delete start!')\n",
    "        df = self.issue_delete(df)\n",
    "        print('issue_delete finish!')\n",
    "        if self.src == 'mimic':\n",
    "            print('name_dict start!')\n",
    "            df = self.name_dict(df)\n",
    "            print('name_dict finish!')\n",
    "        return df, cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class list_preparation():\n",
    "    def __init__(self, df:pd.DataFrame, cohort:pd.DataFrame,  src:str, code:str, file:str, columns_map):\n",
    "        self.columns_map = columns_map\n",
    "        self.file = file\n",
    "        self.df = df.reset_index(drop=True) # our filtered df, now with med names if needed\n",
    "        self.cohort = cohort\n",
    "        self.src = src\n",
    "        if src =='mimic':\n",
    "            self.INTIME = pd.to_datetime(self.cohort['INTIME']) # for manipulation later\n",
    "            self.OUTTIME = pd.to_datetime(self.cohort['OUTTIME']) \n",
    "#####################################################################################################   \n",
    "    def generate_offset(self, item_list:list, df):\n",
    "        '''\n",
    "        Input : time -> output : offset\n",
    "        (order_time - INTIME) and make time as min\n",
    "        \n",
    "        time_list : example [order_time, start_time, end_time ]\n",
    "        offset_list : ex [order_offset, start_offset, end_offset]\n",
    "        '''\n",
    "        self.cohort = self.cohort.reset_index(drop=True)\n",
    "        time_list = ['{}_time'.format(item) for item in item_list] \n",
    "        offset_list = ['{}_offset'.format(item) for item in item_list] # names of cols\n",
    "        offset_dict = {}\n",
    "        index_dict = {}\n",
    "        #Empty dict for offset, empty list for index\n",
    "        for idx, item_time in enumerate(time_list):    # time_list = [start_time, end_time] e.g. for PRESCRIPTIONS\n",
    "            df[item_time] = pd.to_datetime(df[item_time])\n",
    "            offset_dict[offset_list[idx]]=[]\n",
    "            index_list =[]\n",
    "        # from cohort ID, take INTIME\n",
    "        for row, ID in enumerate(notebook.tqdm(self.cohort['ID'])): # w/ progress bar\n",
    "            one_id_rows = df[df['ID']==ID]        \n",
    "            index_list.extend(list(one_id_rows.index)) # add index of one_id_rows\n",
    "            for idx, item_time in enumerate(time_list):\n",
    "                offset_series=one_id_rows[item_time].apply(lambda x: round((x-self.INTIME[row]).total_seconds()/60))               \n",
    "                offset_dict[offset_list[idx]].extend(list(offset_series))\n",
    "        \n",
    "        df= pd.concat([df, pd.DataFrame(offset_dict, index=index_list, columns=offset_list)], axis=1).reset_index(drop=True)\n",
    "        \n",
    "        return df        \n",
    "    \n",
    "    def time_filtering(self, item, df):\n",
    "        ''' \n",
    "        Input : time -> output : timne\n",
    "        time filtering by INTIME , OUTTIME\n",
    "        '''\n",
    "        series = pd.Series()\n",
    "        for row, ID in enumerate(notebook.tqdm(self.cohort['ID'])):\n",
    "            timestamp_in = self.INTIME[row]\n",
    "            timestamp_out = self.OUTTIME[row]\n",
    "            times = df.loc[df['ID'] == ID][item]  # from <  for item in ['start_time','end_time'] >\n",
    "            series = series.append((timestamp_in <= times) & (times <= timestamp_out)) # T/F\n",
    "        df = df.loc[series].reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "############################################################################################################\n",
    "    \n",
    "    def charttime_offset(self, df):\n",
    "        if 'start_time' in df.columns:\n",
    "            df['order_time'] = pd.to_datetime(df['start_time']) #start_time 을 order_time으로\n",
    "            df = self.generate_offset(['order', 'start', 'end'], df)\n",
    "        \n",
    "        elif 'order_time' in df.columns:    \n",
    "            df = self.generate_offset(['order'], df)        \n",
    "        \n",
    "        return df.reset_index(drop=True)    \n",
    "       \n",
    "   \n",
    "    def time_filter(self, df):   \n",
    "        if self.src == 'mimic':\n",
    "            if 'start_time' in df.columns:\n",
    "                for item in ['start_time','end_time']:\n",
    "                    df[item] = pd.to_datetime(df[item])\n",
    "                    df = self.time_filtering(item, df)\n",
    "            elif 'order_time' in df.columns:\n",
    "                    df['order_time'] = pd.to_datetime(df['order_time'])\n",
    "                    df = self.time_filtering('order_time', df)          \n",
    "    \n",
    "        elif self.src == 'eicu': \n",
    "            if 'order_offset' in df.columns:\n",
    "                df = df.loc[df['order_offset'] >= 0].reset_index(drop=True)   \n",
    "            elif 'start_offset' in df.columns:\n",
    "                #start_offset -> order_offset copy\n",
    "                df = df.loc[df['start_offset'] >= 0].reset_index(drop=True)\n",
    "                df = df.loc[df['end_offset'] >= df['start_offset']].reset_index(drop=True)\n",
    "                df['order_offset'] = df['start_offset']\n",
    "                  \n",
    "        \n",
    "        return df.reset_index(drop=True)    \n",
    "\n",
    "    def list_make_sort(self, df):\n",
    "            cohort = self.cohort.reset_index(drop=True)  \n",
    "            columns = self.columns_map[self.file]\n",
    "            columns_names = [value for key, value in columns.items() if value not in ['order_offset','ID','order_time','issue']]\n",
    "            columns_names.append('order_offset') # take .csv file columns + order_offset\n",
    "            columns_dict = {}\n",
    "            for column in columns_names:\n",
    "                columns_dict[column]=[]# dictionary w/ column names as keys\n",
    "                if df[column].dtype =='O':\n",
    "                    df[column] = df[column].str.lower()\n",
    "            for row, ID in enumerate(notebook.tqdm(self.cohort['ID'])):\n",
    "                one_id_rows = df.loc[df['ID']==ID] # take each ID\n",
    "                sort_by_offset = one_id_rows.sort_values(by='order_offset', ascending=True) # sort events by time since admission\n",
    "                for column in columns_names:\n",
    "                    columns_dict[column].append(list(sort_by_offset[column].values)) # append these as value list to key\n",
    "                \n",
    "            for column in columns_names:\n",
    "                list_column=columns_dict[column] # take the key:value pairs one by one \n",
    "                series_list = pd.Series(list_column) # make it a series \n",
    "                cohort[column] = series_list # append it to cohort dataframe as a new column w/ same name as in df\n",
    "            \n",
    "            return cohort\n",
    "    \n",
    "    def __call__(self):        \n",
    "        print('time_filter start!')\n",
    "        df = self.time_filter(self.df)\n",
    "        print('time_filter finished!')\n",
    "        if self.src == 'mimic':\n",
    "            print('charttime_offset start!')\n",
    "            df = self.charttime_offset(df)   \n",
    "            print('charttime_offset finished!')\n",
    "        print('list_make_sort start!')  \n",
    "        cohort = self.list_make_sort(df)\n",
    "        print('list_make_sort finished!')\n",
    "        return cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medication align & INPUTEVENTS Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1: Concat mimic med\n",
    "#mimic : code_name, prod, value, value_uom, route\n",
    "def medication_align(src, med):\n",
    "    mimic_list = ['code_name', 'prod', 'route']\n",
    "    eicu_list = ['code_name', 'route']\n",
    "    if src == 'mimic':\n",
    "        for column in mimic_list:\n",
    "            med_list = []\n",
    "            for column_list in med[column]: \n",
    "                revised_column_list = ['[UNK]' if x=='null' else x for x in column_list]\n",
    "                med_list.append(revised_column_list)\n",
    "            med[column] = pd.Series(med_list)\n",
    "        med = med.rename(columns={'code_name':'code_name_old'})\n",
    "        med['code_name'] = pd.Series([[i+' '+j+' '+k for i,j,k in zip(med['code_name_old'].iloc[z], \\\n",
    "                                                                             med['prod'].iloc[z], \\\n",
    "                                                                             med['route'].iloc[z])] \\\n",
    "                                       for z in range(len(med.index))])\n",
    "    elif src =='eicu':\n",
    "        for column in eicu_list:\n",
    "            med_list = []\n",
    "        for column_list in med[column]: \n",
    "            revised_column_list = ['[UNK]' if x=='null' else x for x in column_list]\n",
    "            med_list.append(revised_column_list)\n",
    "        med[column] = pd.Series(med_list)\n",
    "        med = med.rename(columns={'code_name':'code_name_old'})\n",
    "        med['code_name'] = pd.Series([[i+' '+j for i,j in zip(med['code_name_old'].iloc[z], \\\n",
    "                                                                         med['route'].iloc[z])] \\\n",
    "                                    for z in range(len(med.index))])\n",
    "\n",
    "   \n",
    "    med = med.drop('code_name_old', axis=1)\n",
    "    print('Finished mimic med code_name/amount/units concatenation.')\n",
    "    return med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge INPUTEVENTS _CV and _MV\n",
    "#exist_output = os.listdir(output_dir) # what's in our output directory?\n",
    "#print('exist_files!',exist_output)\n",
    "def Merge_INPUTEVENTS(ie_cv, ie_mv):  \n",
    "    #if file_name_merged not in exist_output:\n",
    "    print('Beginning INPUTEVENTS CV-MV merge...')\n",
    "    print('ie_cv length', len(ie_cv))\n",
    "    print('ie_mv length', len(ie_mv)) \n",
    "    concat = pd.concat([ie_cv, ie_mv]).reset_index(drop=True)\n",
    "    print('Files concatenated.')\n",
    "    print('merged length', len(concat))\n",
    "    \n",
    "    # General practice for concatenating: check columns\n",
    "    not_intersection_mv = [col for col in concat.columns if col not in ie_mv.columns]\n",
    "    not_intersection_cv = [col for col in concat.columns if col not in ie_cv.columns]\n",
    "\n",
    "    if not_intersection_mv != [] or not_intersection_cv != []:\n",
    "        print('Not all columns in new file shared between original files.')\n",
    "        print(' Cols not in CV: ', not_intersection_cv, '\\n Cols not in MV: ', not_intersection_mv)\n",
    "\n",
    "    # Uncomment if part (3) is not being run    \n",
    "    # concat.to_pickle(os.path.join(output_dir, 'mimic_inf_INPUTEVENTS_merged_init.pkl'))\n",
    "    # print('Output file: < mimic_inf_INPUTEVENTS_merged_init.pkl > to ', output_dir)\n",
    "\n",
    "    print('Finished INPUTEVENTS CV-MV concatenation.')\n",
    "\n",
    "\n",
    "\n",
    "    ### 3: Process INPUTEVENTS from MIMIC\n",
    "\n",
    "    start = time.time()\n",
    "    print('Beginning INPUTEVENTS_merged code_name processing...')\n",
    "\n",
    "    concat['code_name_new'] = pd.Series([[(str(i)+' ' \\\n",
    "                                               +('('+str(j)+')').replace('(null)','')+' '\\\n",
    "                                               +('('+str(k)+')').replace('(null)','')).replace('  ',' ').strip() \\\n",
    "                                  for i,j,k in zip(concat['code_name'].iloc[z], \\\n",
    "                                                  concat['value_uom'].iloc[z], \\\n",
    "                                                  concat['rateuom'].iloc[z])] \\\n",
    "                                   for z in range(len(concat.index))])\n",
    "    concat = concat.drop('code_name', axis=1)\n",
    "    concat = concat.rename(columns={'code_name_new':'code_name'})\n",
    "    # for nan \n",
    "    inf_code_list = []\n",
    "    for code_list in concat['code_name']: \n",
    "        revised_code_list = ['[UNK]' if x[0:3]=='nan' else x for x in code_list]\n",
    "        inf_code_list.append(revised_code_list)\n",
    "    concat['code_name'] = pd.Series(inf_code_list)\n",
    "    MV_cv_ID = [181115, 195382,   128403,  100764,  139787,  129547,  120396,  193603, 196357, 156527,150835,156883,137829,120994,\n",
    "                114047,162468,163525,199270,108640,125746,131525,183600,177082,157527,103061,189921,117340,109485,107962,188038,\n",
    "                169344,117448,191001,144919,161350,106153,116756,148722,170826,159412,180006]\n",
    "    print('Remove index', concat[concat['ID'].isin(MV_cv_ID)==True].index)\n",
    "    print('length',len(concat[concat['ID'].isin(MV_cv_ID)==True].index))\n",
    "    concat = concat.drop(concat[concat['ID'].isin(MV_cv_ID)==True].index,axis=0).reset_index(drop=True)\n",
    "    print('Finished INPUTEVENTS_merged code_name processing.')\n",
    "    return concat\n",
    "    #  An alternative idea would have been to process the eICU data to extract the measurement unit information\n",
    "    #  and put it in a separate column as another feature. However, while this is easy for the first 100-200 rows, \n",
    "    #  (as all measurements are contained in parentheses and can be regex-ed out), soon other drug names include\n",
    "    #  the same string pattern for non-measurements, making automatic identification of units quite diffifult. \n",
    "    #  Naive code below:\n",
    "    #\n",
    "    # eicu_inf = pd.read_pickle(os.path.join(input_dir, 'eicu_inf_infusionDrug_init.pkl'))\n",
    "    # test_eicu = eicu_inf.copy()\n",
    "    # test_eicu['code_name_unit'] = pd.Series([[i[i.find('(')+1:i.find(')')] for i in test_eicu['code_name'][k]] \\\n",
    "    #                                   for k in range(len(test_eicu.index))])\n",
    "    # test_eicu['mod_code_name'] = pd.Series([[re.sub(r'\\ \\([^)]*\\)', '', i) for i in test_eicu['code_name'][k]] \\\n",
    "    #                                   for k in range(len(test_eicu.index))])\n",
    "    # units = [i for k in range(len(test_eicu.index)) for i in test_eicu.code_name_unit[k]]\n",
    "    # print(pd.Series(units).unique())\n",
    "    # test_eicu.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single item to Merge items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_item_into_one(src, lab, med ,inf):\n",
    "    print('{} lab item patient numbers .. {} '.format(src,len(lab)))\n",
    "    print('{} med item patient numbers .. {} '.format(src,len(med)))\n",
    "    print('{} inf item patient numbers .. {} '.format(src,len(inf)))\n",
    "    lab.rename(columns = {'code_name':'lab_name','order_offset':'lab_offset'}, inplace=True)\n",
    "    med.rename(columns = {'code_name':'med_name','order_offset':'med_offset'}, inplace=True)\n",
    "    inf.rename(columns = {'code_name':'inf_name','order_offset':'inf_offset'}, inplace=True)\n",
    "    ID_columns = [{'HADM_ID':'ID'}, {'patientunitstayid':'ID'}]\n",
    "    if src == 'mimic':\n",
    "        ID_column = ID_columns[0]\n",
    "    if src == 'eicu':    \n",
    "        ID_column = ID_columns[1]\n",
    "    cohort_path = os.path.join(input_dir, src+'_cohort_dx_done.pk')\n",
    "    cohort = pd.read_pickle(cohort_path).reset_index(drop=True)\n",
    "    cohort.rename(columns = ID_column, inplace=True)\n",
    "    print('cohort patient numbers .. {}'.format(len(cohort)))\n",
    "    cohort= cohort[['ID', 'readmission','mortality','los>3day',\n",
    "                    'los>7day','dx_depth1_unique']]\n",
    "    merged_df = pd.merge(cohort, lab[['ID','lab_name','lab_offset']], on='ID', how='left')\n",
    "    merged_df = pd.merge(merged_df, med[['ID','med_name','med_offset']], on='ID', how='left')\n",
    "    merged_df = pd.merge(merged_df, inf[['ID','inf_name','inf_offset']], on='ID', how='left')\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "    print('{} three items merged patient numbers .. {} '.format(src, len(merged_df)))\n",
    "    one_list=[]\n",
    "    offset_list=[]\n",
    "    item_index_list=[]\n",
    "    lab = merged_df['lab_name'] ; lab_offset = merged_df['lab_offset']\n",
    "    med = merged_df['med_name'] ; med_offset = merged_df['med_offset']\n",
    "    inf= merged_df['inf_name'] ; inf_offset = merged_df['inf_offset']\n",
    "    for i in range(len(merged_df)):\n",
    "        ones = []\n",
    "        offsets = []\n",
    "        item_index = []\n",
    "        if type(lab[i]) != type([]):\n",
    "            labs = []\n",
    "        else :\n",
    "            labs =lab[i]\n",
    "        if type(med[i]) != type([]):\n",
    "            meds = []\n",
    "        else :\n",
    "            meds =med[i]\n",
    "        if type(inf[i]) != type([]):\n",
    "            infs = []\n",
    "        else :\n",
    "            infs =inf[i]\n",
    "        if type(lab_offset[i]) != type([]):\n",
    "            lab_offsets = []\n",
    "        else :\n",
    "            lab_offsets =lab_offset[i]\n",
    "        if type(med_offset[i]) != type([]):\n",
    "            med_offsets = []\n",
    "        else :\n",
    "            med_offsets =med_offset[i]\n",
    "        if type(inf_offset[i]) != type([]):\n",
    "            inf_offsets= []\n",
    "        else :\n",
    "            inf_offsets =inf_offset[i]            \n",
    "        ones.extend(labs)\n",
    "        ones.extend(meds)\n",
    "        ones.extend(infs)\n",
    "        offsets.extend(lab_offsets)\n",
    "        offsets.extend(med_offsets)\n",
    "        offsets.extend(inf_offsets)\n",
    "        item_index.extend (list(np.full((len(labs)),1)))\n",
    "        item_index.extend(list(np.full((len(meds)),2)))\n",
    "        item_index.extend(list(np.full((len(infs)),3)))\n",
    "        merge = zip(ones, offsets, item_index)\n",
    "        sort = sorted(merge, key=lambda merge: merge[1], reverse=False)\n",
    "        one_lists=[x for x,y,z in sort]\n",
    "        one_offsets=[y for x,y,z in sort]\n",
    "        one_item_index = [z for x,y,z in sort]\n",
    "        one_list.append(one_lists)\n",
    "        offset_list.append(one_offsets)\n",
    "        item_index_list.append(one_item_index)\n",
    "    one_name = pd.Series(one_list)\n",
    "    one_offset = pd.Series(offset_list)\n",
    "    one_item_index =pd.Series(item_index_list)\n",
    "    merged_df['code_name'] = one_name\n",
    "    merged_df['order_offset'] = one_offset\n",
    "    merged_df['item_index'] = one_item_index\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(): \n",
    "    def __init__(self, cohort, src:str, item:str, window, UNK, max_len, min_len, min_freq):\n",
    "        self.cohort = cohort\n",
    "        self.src = src\n",
    "        self.UNK = UNK\n",
    "        self.window = window\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.min_freq = min_freq\n",
    "        self.offset = 'order_offset'\n",
    "        self.item = item\n",
    "            \n",
    "       \n",
    "        if self.window=='Total':\n",
    "            self.name_window = '{}_name'.format(self.item)\n",
    "            self.offset_window = self.offset\n",
    "            self.offset_order = 'offset_order'\n",
    "        else:\n",
    "            self.name_window = '{}_name_{}hr'.format(self.item, str(self.window))\n",
    "            self.offset_window = '{}_{}hr'.format(self.offset, str(self.window))\n",
    "            self.offset_order = '{}_offset_order_{}hr'.format(self.item, str(self.window))\n",
    "    \n",
    "    def timeoffset_window(self): \n",
    "        #(input: timeoffset -output:timeoffset_window):\n",
    "        if self.window == 'Total':\n",
    "                pass\n",
    "        else:\n",
    "            offset_window_lst = []\n",
    "            code_name_window_lst = []\n",
    "            item_index_window_lst=[]\n",
    "            for idx, offset_lst in enumerate(self.cohort[self.offset]): # time since order, e.g. [182, 182, 403, 403, 403]\n",
    "                len_offset_window = len([offset for offset in offset_lst if offset < self.window*60]) # how many < max_time \n",
    "                code_name_lst = self.cohort.code_name.iloc[idx] # which medically relevant thing [e.g. Tylenol 500mg, Epinephrine X mL]\n",
    "                if self.item =='all':\n",
    "                    item_index_lst = self.cohort.item_index.iloc[idx]\n",
    "                    item_index_window = item_index_lst[:len_offset_window]\n",
    "                    item_index_window_lst.append(item_index_window)\n",
    "                offset_window = offset_lst[:len_offset_window]\n",
    "                # truncate both at length of offset window\n",
    "                code_name_window = code_name_lst[:len_offset_window]\n",
    "                offset_window_lst.append(offset_window) # our new truncated window\n",
    "                code_name_window_lst.append(code_name_window) # our new truncated codes\n",
    "                \n",
    "            self.cohort[self.name_window] = pd.Series(code_name_window_lst) # add as new column at end\n",
    "            self.cohort[self.offset_window] = pd.Series(offset_window_lst)\n",
    "            if self.item =='all':\n",
    "                self.cohort['item_index'] = pd.Series(item_index_window_lst)\n",
    "        return self.cohort\n",
    "    \n",
    "    def timeoffset_timeorder(self, cohort): \n",
    "        #(input- timeoffset - timeorder)\n",
    "        offset_order_lst = []\n",
    "        for idx, offset in enumerate(cohort[self.offset_window]):\n",
    "            offset_set = list(set(offset)) # create a set from the interable e.g. {122, 232, 444}\n",
    "            offset_set.sort() \n",
    "            order_value = np.arange(1, len(offset_set)+1)\n",
    "            dict_offset = dict(zip(offset_set, order_value)) # create dictionary of \"order\" of events\n",
    "            offset_order = list(pd.Series(offset).map(dict_offset)) \n",
    "            offset_order_lst.append(offset_order)\n",
    "        cohort[self.offset_order] = pd.Series(offset_order_lst) # offset order is new col indicating ordinality\n",
    "        return cohort\n",
    "    \n",
    "    def code_windowed(self, cohort, max_len, min_len):\n",
    "        name_lst= []\n",
    "        offset_lst = []\n",
    "        offset_order_lst = []\n",
    "        item_lst= []\n",
    "        zero_len_idx=[]\n",
    "        for idx, names in enumerate(self.cohort[self.name_window]): # our truncated code_name column\n",
    "            len_name_window=len(names) # how many of these codes in our iteratred row?\n",
    "            if len_name_window > max_len:\n",
    "                 len_name_window = max_len\n",
    "            if len_name_window < min_len:\n",
    "                zero_len_idx.append(idx) \n",
    "            name = names[:len_name_window] # truncate to the max number of codes we're allowing\n",
    "            offset = cohort[self.offset_window].iloc[idx][:len_name_window]\n",
    "            if self.item =='all':\n",
    "                item_index = cohort['item_index'].iloc[idx][:len_name_window]\n",
    "                item_lst.append(item_index)\n",
    "                # ditto for the [132, 132, 144, etc.]\n",
    "           # offset_order = cohort[self.offset_order].iloc[idx][:len_name_window] # ditto to the order of events (1,2,3..)\n",
    "            name_lst.append(name) # build series\n",
    "            offset_lst.append(offset)\n",
    "            #offset_order_lst.append(offset_order)    \n",
    "        cohort[self.name_window] = pd.Series(name_lst) # replace columns from 1st fxn as necessary\n",
    "        cohort[self.offset_window] = pd.Series(offset_lst)\n",
    "        if self.item =='all': \n",
    "            cohort['item_index']=pd.Series(item_lst)\n",
    "        #cohort[self.offset_order] = pd.Series(offset_order_lst)\n",
    "        \n",
    "        self.cohort = self.cohort.drop(self.cohort.index[[zero_len_idx]]).reset_index(drop=True) # drop if not enough time obs\n",
    "        return self.cohort\n",
    "    \n",
    "                \n",
    "    def make_vocab(self, cohort, min_freq, PAD_idx=0, UNK_idx=1, MASK_idx=2, SEP_idx=3): \n",
    "        #(Input codes output vocab with PAD 0 UNK 1 MASK 2 SEP 3)\n",
    "        #2 options : delete UNK (min_freq) : False or treat min_freq as  UNK : True\n",
    "       \n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        word_freq = dict(Counter(flatten(cohort[self.name_window])))\n",
    "        \n",
    "        if self.UNK == True:\n",
    "            word2idx = {'<PAD>': PAD_idx, '<UNK>': UNK_idx, '<MASK>':MASK_idx, '<SEP>':SEP_idx} \n",
    "            \n",
    "        elif self.UNK == False:\n",
    "            word2idx = {'<PAD>': PAD_idx, '<MASK>':MASK_idx, '<SEP>':SEP_idx}\n",
    "        \n",
    "\n",
    "        min_freq_id=[]\n",
    "        for word_item in word_freq.items():\n",
    "            if word_item[0] not in word2idx:\n",
    "                if word_item[1] < min_freq:\n",
    "                    min_freq_id.append(word_item[0])\n",
    "                    if self.UNK== True:\n",
    "                        word2idx[word_item[0]]=1 #UNK 처리 \n",
    "                else:\n",
    "                    word2idx[word_item[0]] = max(word2idx.values())+1\n",
    "                    \n",
    "        return word2idx, min_freq_id, word_freq\n",
    "    \n",
    "    def code_to_index(self, cohort, word2idx, min_freq_id):\n",
    "        #deleting min_freq word\n",
    "        if self.UNK == False:\n",
    "            dict_del={}\n",
    "            for idx, name_lst in enumerate(cohort[self.name_window]):\n",
    "                del_index=[i for i in range(len(name_lst)) if name_lst[i] in min_freq_id]              \n",
    "                if len(del_index)>0:\n",
    "                    dict_del[idx]=del_index \n",
    "            for idx, order in (dict_del.items()):\n",
    "                item_deleted = [i for j, i in enumerate(cohort[self.name_window][idx]) if j not in order]\n",
    "                offset_deleted = [i for j, i in enumerate(cohort[self.offset_window][idx]) if j not in order]\n",
    "                if self.item =='all':\n",
    "                    item_index_deleted = [i for j,i in enumerate(cohort['item_index'][idx]) if j not in order]\n",
    "            #  offset_order_deleted= [i for j, i in enumerate(cohort[self.offset_order][idx]) if j not in order]\n",
    "             #value_deleted\n",
    "                #measure_deleted\n",
    "                cohort[self.name_window].iloc[idx] = item_deleted\n",
    "                cohort[self.offset_window].iloc[idx] = offset_deleted\n",
    "                if self.item =='all':\n",
    "                    cohort['item_index'].iloc[idx] = item_index_deleted\n",
    "               # cohort[self.offset_order].iloc[idx] = offset_order_deleted\n",
    "        #mapping\n",
    "        item_id=[]    \n",
    "        for name_lst in cohort[self.name_window]:\n",
    "            item_id_lst=list(pd.Series(name_lst).map(word2idx))\n",
    "            item_id.append(item_id_lst)\n",
    "        item_id = pd.Series(item_id)\n",
    "        cohort['{}_id_{}hr'.format(self.item, str(self.window))]=item_id\n",
    "        \n",
    "        return cohort\n",
    "     \n",
    "    def arguments(self):\n",
    "        return  [self.src, self.window, self.item]\n",
    "    \n",
    "    def __call__(self):\n",
    "        cohort = self.timeoffset_window()\n",
    "        print('timeoffset windowing finish!')\n",
    "        cohort = self.code_windowed(cohort, self.max_len, self.min_len)\n",
    "        print('Patient filtering by code length finish!')\n",
    "        word2idx, min_freq_id, word_freq = self.make_vocab(cohort, min_freq=self.min_freq, PAD_idx=0, UNK_idx=1, MASK_idx=2, SEP_idx=3)\n",
    "        print('Vocab making finish!')\n",
    "        cohort = self.code_to_index(cohort, word2idx, min_freq_id)\n",
    "        cohort = self.timeoffset_timeorder(cohort)\n",
    "        return cohort, word2idx, min_freq_id, word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'mimic'\n",
    "item='all'\n",
    "seed = 2020\n",
    "columns_map = mimic_columns_map\n",
    "window_time = 12\n",
    "UNK = False\n",
    "max_len = 150\n",
    "min_len = 5\n",
    "min_freq = 5\n",
    "item_list =['lab','med','inf']\n",
    "source_list = ['mimic','eicu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPreparation csv file into csv_dir : MIMIC-III : \\n                                    eICU : \\nPreprocess steps :  1. Preprocess_1 (data_init, list_preparation)  \\n                    2. Intermediate step_1(medication align & INPUTEVENTS MERGE) \\n                    3. Intermediate step_2(Three item merged)\\n                    4. Preprocess_2 (make vocab, pruning patient)\\n'It will take more than 30mins'\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Preparation csv file into csv_dir : MIMIC-III : \n",
    "                                    eICU : \n",
    "Preprocess steps :  1. Preprocess_1 (data_init, list_preparation)  \n",
    "                    2. Intermediate step_1(medication align & INPUTEVENTS MERGE) \n",
    "                    3. Intermediate step_2(Three item merged)\n",
    "                    4. Preprocess_2 (make vocab, pruning patient)\n",
    "'It will take more than 30mins'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preparation initialization .. mimic lab\n",
      "cohort load finish!\n",
      "csv file load finish!\n",
      "column_rename start!\n",
      "column_rename finish!\n",
      "cohort_filtering start!\n",
      "cohort_filtering finish!\n",
      "issue_delete start!\n",
      "issue_delete finish!\n",
      "name_dict start!\n",
      "name_dict finish!\n",
      "time_filter start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-539a2e7caad1>:47: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  series = pd.Series()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac85d7f3a63446df8b8b41d173b3edf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time_filter finished!\n",
      "charttime_offset start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aa1c67f3cd4d13bff4ffa39b5f6961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "charttime_offset finished!\n",
      "list_make_sort start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85650e5c6ba14c3b8400526e2ede360c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list_make_sort finished!\n",
      "data preparation initialization .. mimic med\n",
      "cohort load finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file load finish!\n",
      "column_rename start!\n",
      "column_rename finish!\n",
      "cohort_filtering start!\n",
      "cohort_filtering finish!\n",
      "issue_delete start!\n",
      "issue_delete finish!\n",
      "name_dict start!\n",
      "name_dict finish!\n",
      "time_filter start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-539a2e7caad1>:47: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  series = pd.Series()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f94c5d12dda466f95b4f7be0ec7e524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17578.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time_filter finished!\n",
      "charttime_offset start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac7b34edd28497ea5ba9f8806c4c72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17578.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "charttime_offset finished!\n",
      "list_make_sort start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a91b1e83bc4972b2e65d68e4b1abd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17578.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list_make_sort finished!\n",
      "Finished mimic med code_name/amount/units concatenation.\n",
      "data preparation initialization .. mimic inf\n",
      "cohort load finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (7,9,17,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file load finish!\n",
      "column_rename start!\n",
      "column_rename finish!\n",
      "cohort_filtering start!\n",
      "cohort_filtering finish!\n",
      "issue_delete start!\n",
      "issue_delete finish!\n",
      "name_dict start!\n",
      "name_dict finish!\n",
      "time_filter start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-539a2e7caad1>:47: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  series = pd.Series()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfd4fbbaf454cd7ba2e02698b08088d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9366.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time_filter finished!\n",
      "charttime_offset start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65336f55657941da8f33079a3b04835e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9366.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "charttime_offset finished!\n",
      "list_make_sort start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082197a008a544e5945cb347f26db47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9366.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list_make_sort finished!\n",
      "data preparation initialization .. mimic inf\n",
      "cohort load finish!\n",
      "csv file load finish!\n",
      "column_rename start!\n",
      "column_rename finish!\n",
      "cohort_filtering start!\n",
      "cohort_filtering finish!\n",
      "issue_delete start!\n",
      "issue_delete finish!\n",
      "name_dict start!\n",
      "name_dict finish!\n",
      "time_filter start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-539a2e7caad1>:47: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  series = pd.Series()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838ad258f1734517b6c81a92859e9de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time_filter finished!\n",
      "charttime_offset start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0700f422e5475f93dfe9748e691fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "charttime_offset finished!\n",
      "list_make_sort start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d952da0e2a8f423d950c426684ba332e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list_make_sort finished!\n",
      "Beginning INPUTEVENTS CV-MV merge...\n",
      "ie_cv length 9366\n",
      "ie_mv length 9125\n",
      "Files concatenated.\n",
      "merged length 18491\n",
      "Finished INPUTEVENTS CV-MV concatenation.\n",
      "Beginning INPUTEVENTS_merged code_name processing...\n",
      "index Int64Index([   80,   286,   589,   790,   866,   957,  1369,  1613,  1683,\n",
      "             1692,  1951,  1998,  2459,  2711,  2819,  2998,  3582,  3764,\n",
      "             4216,  4532,  4763,  5294,  5322,  5387,  5542,  5730,  5833,\n",
      "             5934,  6476,  6607,  7202,  7474,  7585,  7807,  8252,  8436,\n",
      "             8538,  8781,  8940,  9024,  9308,  9433,  9634,  9914, 10060,\n",
      "            10100, 10186, 10595, 10832, 10905, 10914, 11176, 11221, 11644,\n",
      "            11916, 12009, 12190, 12759, 12940, 13375, 13729, 13926, 14468,\n",
      "            14504, 14573, 14725, 14904, 15002, 15112, 15667, 15821, 16420,\n",
      "            16688, 16788, 17016, 17388, 17559, 17664, 17897, 18059, 18163,\n",
      "            18416],\n",
      "           dtype='int64')\n",
      "length 82\n",
      "Finished INPUTEVENTS_merged code_name processing.\n",
      "data preparation finish for three items \n",
      " second preparation start soon..\n",
      "preprocessing for first part.. mimic_inf_INPUTEVENTS_MV finish!\n",
      "mimic lab item patient numbers .. 18625 \n",
      "mimic med item patient numbers .. 17578 \n",
      "mimic inf item patient numbers .. 18409 \n",
      "cohort patient numbers .. 18799\n",
      "mimic three items merged patient numbers .. 18799 \n",
      "lab med inf three categories merged in one!\n",
      "2nd Preprocessing start!...\n",
      "timeoffset windowing finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexes/base.py:4111: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient filtering by code length finish!\n",
      "Vocab making finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "Writing mimic_12_inf_150.pkl to ./input\n",
      "Generated vocabulary of length 2858 \n",
      "\n",
      "data preparation initialization .. eicu lab\n",
      "cohort load finish!\n",
      "csv file load finish!\n",
      "column_rename start!\n",
      "column_rename finish!\n",
      "cohort_filtering start!\n",
      "cohort_filtering finish!\n",
      "issue_delete start!\n",
      "issue_delete finish!\n",
      "time_filter start!\n",
      "time_filter finished!\n",
      "list_make_sort start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b925057f464b59bc11fdb0fa38ca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13249.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list_make_sort finished!\n",
      "data preparation initialization .. eicu med\n",
      "cohort load finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file load finish!\n",
      "column_rename start!\n",
      "column_rename finish!\n",
      "cohort_filtering start!\n",
      "cohort_filtering finish!\n",
      "issue_delete start!\n",
      "issue_delete finish!\n",
      "time_filter start!\n",
      "time_filter finished!\n",
      "list_make_sort start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f77e205f4b48089949ec0c2da7bee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=11710.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list_make_sort finished!\n",
      "Finished mimic med code_name/amount/units concatenation.\n",
      "data preparation initialization .. eicu inf\n",
      "cohort load finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file load finish!\n",
      "column_rename start!\n",
      "column_rename finish!\n",
      "cohort_filtering start!\n",
      "cohort_filtering finish!\n",
      "issue_delete start!\n",
      "issue_delete finish!\n",
      "time_filter start!\n",
      "time_filter finished!\n",
      "list_make_sort start!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4262fb717e054716b7598df97f52bb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6273.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "list_make_sort finished!\n",
      "data preparation finish for three items \n",
      " second preparation start soon..\n",
      "preprocessing for first part.. eicu_inf_infusionDrug finish!\n",
      "eicu lab item patient numbers .. 13249 \n",
      "eicu med item patient numbers .. 11710 \n",
      "eicu inf item patient numbers .. 6273 \n",
      "cohort patient numbers .. 13364\n",
      "eicu three items merged patient numbers .. 13364 \n",
      "lab med inf three categories merged in one!\n",
      "2nd Preprocessing start!...\n",
      "timeoffset windowing finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexes/base.py:4111: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient filtering by code length finish!\n",
      "Vocab making finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed.\n",
      "Writing eicu_12_inf_150.pkl to ./input\n",
      "Generated vocabulary of length 2021 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for src in source_list:\n",
    "    for item in item_list:\n",
    "        if src == 'mimic':\n",
    "            files = mimic_csv_files[item] # the files from mimic that we want\n",
    "            columns_map= mimic_columns_map # the columns from mimic we care about, to be arg for pre_processing_1st\n",
    "        elif src == 'eicu':     \n",
    "            files = eicu_csv_files[item]\n",
    "            columns_map= eicu_columns_map \n",
    "        for file in files:\n",
    "            print('data preparation initialization .. {} {}'.format(src, item))\n",
    "            data = data_init(file, src, item, columns_map)\n",
    "            df, cohort = data()\n",
    "            list_prep = list_preparation(df, cohort, src, item, file, columns_map)\n",
    "            pickle_cohort= list_prep()\n",
    "            if item == 'lab':\n",
    "                lab = pickle_cohort.copy()\n",
    "            elif item =='med':\n",
    "                med = pickle_cohort.copy()\n",
    "                med = medication_align(src, med)\n",
    "            elif item =='inf':\n",
    "                if src =='mimic':\n",
    "                    if file =='INPUTEVENTS_CV':\n",
    "                        ie_cv = pickle_cohort.copy()\n",
    "                    elif file == 'INPUTEVENTS_MV':\n",
    "                        ie_mv = pickle_cohort.copy()\n",
    "                        inf = Merge_INPUTEVENTS(ie_cv, ie_mv)\n",
    "                elif src=='eicu':\n",
    "                    inf=pickle_cohort.copy()\n",
    "    print('data preparation finish for three items \\n second preparation start soon..')\n",
    "    print('preprocessing for first part.. {}_{}_{} finish!'.format(src,item,file))\n",
    "    merged_df =multiple_item_into_one(src, lab, med ,inf)\n",
    "    merged_df= merged_df.drop(columns=['lab_name', 'lab_offset', 'med_name', 'med_offset','inf_name','inf_offset'])\n",
    "    print('lab med inf three categories merged in one!')\n",
    "    print('2nd Preprocessing start!...') \n",
    "    second_preprocess = Preprocess(merged_df, src, item, window_time, UNK, max_len, min_len, min_freq)\n",
    "    cohort, vocab, min_freq_id, word_freq = second_preprocess()\n",
    "    print('Preprocessing completed.')\n",
    "    print('Writing', '{}_{}_{}_{}.pkl'.format(src, window_time, 'all', max_len), 'to', input_dir)\n",
    "    cohort.to_pickle(os.path.join(input_dir,'{}_{}_{}_{}.pkl'.format(src, window_time, all, max_len)))\n",
    "    print('Generated vocabulary of length', len(vocab), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & valid & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(target_cols, cohort, random_state, test_ratio, train_valid_fold, columns):\n",
    "    sss_train_test = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state =random_state)\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=train_valid_fold, random_state=random_state)\n",
    "    X = cohort.copy()\n",
    "    for target in target_cols:\n",
    "        if target == 'dx':\n",
    "            continue\n",
    "        print(\"____{}____ train and test split start!\".format(target))\n",
    "        fold_column = '{}_fold'.format(target)\n",
    "        X[fold_column]=1\n",
    "        print('X fold_column \\n',X[fold_column].value_counts())\n",
    "        y = X[target]\n",
    "\n",
    "        #train / test split 4:1\n",
    "        for train_index, test_index in sss_train_test.split(X,y):\n",
    "            X_test = X.loc[test_index]\n",
    "            X_test.loc[test_index, fold_column]=0    \n",
    "\n",
    "        #train = -1 , test = 0\n",
    "        X_train = X.loc[train_index].reset_index(drop=True)\n",
    "        y_train = y.loc[train_index].reset_index(drop=True)\n",
    "\n",
    "        #train / valid slpit 4:1 \n",
    "        for train_index, valid_index in sss_train_test.split(X_train,y_train):\n",
    "            X_valid = X_train.loc[valid_index]\n",
    "            X_valid.loc[valid_index, fold_column]=2\n",
    "\n",
    "\n",
    "        #test = 0 train = 1 valid = 2\n",
    "        X_train = X_train.loc[train_index].reset_index(drop=True)\n",
    "        X_valid = X_valid.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "        X = pd.concat([X_train, X_valid, X_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "        print('fold_split_results!!!!! \\n', X[fold_column].value_counts())\n",
    "\n",
    "        '''  \n",
    "        Multi_label_stratified_Kfold\n",
    "        ''' \n",
    "    #diagnosis multi_label stratified_Kfold\n",
    "    print('___diagnosis multi_label_stratified_split____') \n",
    "    X['dx_fold'] = 1\n",
    "    y = X[columns]\n",
    "    for i, (train_index, test_index) in enumerate(mskf.split(X,y)):\n",
    "        if i != 0 :\n",
    "            continue\n",
    "        elif i== 0:\n",
    "            trn_index = train_index\n",
    "            X_test=X.loc[test_index]\n",
    "            X_test.loc[test_index, 'dx_fold'] = 0\n",
    "\n",
    "    #X_train \n",
    "    X_train = X.loc[trn_index].reset_index(drop=True)\n",
    "    y_train = y.loc[trn_index].reset_index(drop=True)\n",
    "    for i, (train_index, valid_index) in enumerate(mskf.split(X_train, y_train)):\n",
    "        if i != 0 :\n",
    "            continue\n",
    "        elif i== 0:\n",
    "            trn_index = train_index\n",
    "            X_valid= X_train.loc[valid_index]\n",
    "            X_valid.loc[valid_index, 'dx_fold'] = 2\n",
    "\n",
    "    #test = 0 train = 1 valid = 2\n",
    "    X_train = X_train.loc[trn_index].reset_index(drop=True)\n",
    "    X_valid = X_valid.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    X = pd.concat([X_train, X_valid,X_test]).reset_index(drop=True)\n",
    "    print('fold_split_results!!!!! \\n', X['dx_fold'].value_counts())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx_one_hot_encoding(cohort):\n",
    "    #dx code -> one-hot encoding\n",
    "    dx_label_criteria = 'dx_depth1_unique'\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    encoded_dx = mlb.fit_transform(cohort[dx_label_criteria])\n",
    "    columns=mlb.classes_\n",
    "    df_dx = pd.DataFrame(encoded_dx, columns=columns)\n",
    "    cohort = pd.concat([cohort, df_dx],axis=1)\n",
    "    return cohort, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = ['mimic','eicu']\n",
    "target_cols = ['readmission', 'mortality', 'los>3day', 'los>7day']\n",
    "random_state_list = [2020,2021,2022,2023,2024,2025,2026,2027,2028,2029]\n",
    "# train / test ratio\n",
    "test_ratio = 0.2\n",
    "# train/ valid folds\n",
    "train_valid_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=2020 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____readmission____ train and test split start!\n",
      "X fold_column \n",
      " 1    18536\n",
      "Name: readmission_fold, dtype: int64\n",
      "fold_split_results!!!!! \n",
      " 1    11862\n",
      "0     3708\n",
      "2     2966\n",
      "Name: readmission_fold, dtype: int64\n",
      "____mortality____ train and test split start!\n",
      "X fold_column \n",
      " 1    18536\n",
      "Name: mortality_fold, dtype: int64\n",
      "fold_split_results!!!!! \n",
      " 1    11862\n",
      "0     3708\n",
      "2     2966\n",
      "Name: mortality_fold, dtype: int64\n",
      "____los>3day____ train and test split start!\n",
      "X fold_column \n",
      " 1    18536\n",
      "Name: los>3day_fold, dtype: int64\n",
      "fold_split_results!!!!! \n",
      " 1    11862\n",
      "0     3708\n",
      "2     2966\n",
      "Name: los>3day_fold, dtype: int64\n",
      "____los>7day____ train and test split start!\n",
      "X fold_column \n",
      " 1    18536\n",
      "Name: los>7day_fold, dtype: int64\n",
      "fold_split_results!!!!! \n",
      " 1    11862\n",
      "0     3708\n",
      "2     2966\n",
      "Name: los>7day_fold, dtype: int64\n",
      "___diagnosis multi_label_stratified_split____\n",
      "fold_split_results!!!!! \n",
      " 1    11856\n",
      "0     3708\n",
      "2     2972\n",
      "Name: dx_fold, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-bb312e6ef14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcohort_fold_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcohort_fold_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}_12_all_150_{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcohort_fold_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol)\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2676\u001b[0;31m         \u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m     def to_clipboard(\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2015\u001b[0m     \u001b[0;31m# Pickle Methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2017\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2018\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attributes_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for src in source_list:\n",
    "    cohort = pd.read_pickle(os.path.join(input_dir, '{}_12_all_150.pkl'.format(src))).reset_index(drop=True)\n",
    "    cohort, columns = dx_one_hot_encoding(cohort)\n",
    "    for random_state in random_state_list:\n",
    "        cohort_fold_split =train_valid_test_split(target_cols, cohort, random_state, test_ratio, train_valid_fold, columns)\n",
    "        cohort_fold_split= cohort_fold_split.drop(columns=columns)\n",
    "        save_file = '{}_12_all_150_{}.pkl'.format(src, random_state)\n",
    "        print('save to .. ', os.path.join(input_dir, save_file))\n",
    "        cohort_fold_split.to_pickle(os.path.join(input_dir, save_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fewshot sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fewshot_sampling(target_cols, cohort, random_state, sampled_ratio, sampling_fold,train_valid_fold, columns):\n",
    "    sss_sampling = StratifiedShuffleSplit(n_splits=1, test_size=sampled_ratio, random_state =random_state)\n",
    "    sss=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state =random_state)\n",
    "    mskf_sampling = MultilabelStratifiedKFold(n_splits=10, random_state=random_state)\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, random_state=random_state)\n",
    "    X = cohort.copy()\n",
    "    print('len(cohort)',len(X))\n",
    "    print('sampling ratio', sampled_ratio)\n",
    "    for target in target_cols:\n",
    "        print(\"____{}____ train and test split start!\".format(target))\n",
    "        fold_column = '{}_fold'.format(target)\n",
    "        #print('X fold_column \\n',X[fold_column].value_counts())\n",
    "        X_test = X.loc[X[fold_column]==0].reset_index(drop=True)\n",
    "        X_train = X.loc[X[fold_column].isin([1,2])]\n",
    "        #print('X_train \\n' ,X_train[fold_column].value_counts())\n",
    "        y_train = X_train[target].reset_index(drop=True)\n",
    "        X_train[fold_column] = -1\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        \n",
    "        #Sampling from X_train/valid\n",
    "        for unsampled_train_index, sampled_train_index in sss_sampling.split(X_train,y_train):\n",
    "            X_unsampled=X_train.loc[unsampled_train_index].reset_index(drop=True)\n",
    "            X_sampled = X_train.loc[sampled_train_index]\n",
    "            X_sampled.loc[sampled_train_index, fold_column]=1\n",
    "            X_sampled=X_sampled.reset_index(drop=True)\n",
    "            y_sampled = y_train.loc[sampled_train_index].reset_index(drop=True)\n",
    "        \n",
    "        #train = 1 , valid = 2, test = 0\n",
    "       # print('X_sampled \\n', X_sampled[fold_column].value_counts())\n",
    "        for train_index, valid_index in sss.split(X_sampled,y_sampled):\n",
    "            X_valid = X_sampled.loc[valid_index]\n",
    "            X_valid.loc[valid_index, fold_column]=2\n",
    "            X_train = X_sampled.loc[train_index]\n",
    "\n",
    "            \n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_valid = X_valid.reset_index(drop=True)\n",
    "\n",
    "        X = pd.concat([X_train, X_unsampled, X_valid, X_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "        print('fold_split_results!!!!! \\n', X[fold_column].value_counts())\n",
    "    \n",
    "    '''  \n",
    "    Multi_label_stratified_Kfold\n",
    "    ''' \n",
    "    #diagnosis multi_label stratified_Kfold\n",
    "    print('___diagnosis multi_label_stratified_split____') \n",
    "    X_test = X.loc[X['dx_fold']==0].reset_index(drop=True)\n",
    "    #print('X_test',len(X_test))\n",
    "    X_train = X.loc[X['dx_fold'].isin([1,2])].reset_index(drop=True)\n",
    "    X_train['dx_fold'] = -1\n",
    "    y_train = X_train[columns].reset_index(drop=True)\n",
    "   # print('X_train',len(X_train))\n",
    "    for i, (unsampled_train_index, sampled_train_index) in enumerate(mskf_sampling.split(X_train,y_train)):\n",
    "            X_train.loc[sampled_train_index, 'dx_fold']=i+1\n",
    "\n",
    "    sampling_i = [i+1 for i in range(sampling_fold)]\n",
    "    X_unsampled = X_train.loc[~X_train['dx_fold'].isin(sampling_i)].reset_index(drop=True)\n",
    "    X_unsampled['dx_fold']=-1\n",
    "    #print('samping_i', sampling_i)\n",
    "    #print('X_unsampled', X_unsampled['dx_fold'].value_counts())\n",
    "    X_sampled = X_train.loc[X_train['dx_fold'].isin(sampling_i)]\n",
    "    X_sampled['dx_fold'] = 1\n",
    "    #print('index len' ,len(X_sampled.index))\n",
    "    y_sampled = y_train.loc[X_sampled.index].reset_index(drop=True)\n",
    "    X_sampled = X_sampled.reset_index(drop=True)\n",
    "    #print('X_sampled',len(X_sampled))        \n",
    "    \n",
    "    #print('X_sampled',X_sampled['dx_fold'].value_counts())\n",
    "    for i, (train_index, valid_index) in enumerate(mskf.split(X_sampled, y_sampled)):\n",
    "        if i != 0 :\n",
    "            continue\n",
    "        elif i==0:\n",
    "            X_valid = X_sampled.loc[valid_index]\n",
    "            X_valid.loc[valid_index, 'dx_fold']=2\n",
    "            X_train = X_sampled.loc[train_index]\n",
    "    \n",
    "    #test = 0 train = 1 valid = 2\n",
    "    X = pd.concat([X_train, X_unsampled, X_valid,X_test]).reset_index(drop=True)\n",
    "    print('fold_split_results!!!!! \\n', X['dx_fold'].value_counts())\n",
    "    \n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = ['mimic','eicu']\n",
    "items = ['all']\n",
    "time_window=12\n",
    "max_len =150\n",
    "target_cols = ['readmission', 'mortality', 'los>3day', 'los>7day']\n",
    "random_state_list = [2020,2021,2022,2023,2024,2025,2026,2027,2028,2029]\n",
    "# train/ valid folds 4:1\n",
    "train_valid_fold = 5\n",
    "sampling_fold_list = [1,3,5,7,9]\n",
    "sampled_ratio_list = [0.1,0.3,0.5,0.7,0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mimic_start!\n",
      "seed_2020\n",
      "read data from /home/ghhur/data/input/mimic_12_all_150_2020.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=2020 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cohort) 18536\n",
      "sampling ratio 0.1\n",
      "____readmission____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    13345\n",
      " 0     3708\n",
      " 1     1186\n",
      " 2      297\n",
      "Name: readmission_fold, dtype: int64\n",
      "____mortality____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    13345\n",
      " 0     3708\n",
      " 1     1186\n",
      " 2      297\n",
      "Name: mortality_fold, dtype: int64\n",
      "____los>3day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    13345\n",
      " 0     3708\n",
      " 1     1186\n",
      " 2      297\n",
      "Name: los>3day_fold, dtype: int64\n",
      "____los>7day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    13345\n",
      " 0     3708\n",
      " 1     1186\n",
      " 2      297\n",
      "Name: los>7day_fold, dtype: int64\n",
      "___diagnosis multi_label_stratified_split____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b809d4541ed5>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_sampled['dx_fold'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_split_results!!!!! \n",
      " -1    13354\n",
      " 0     3708\n",
      " 1     1181\n",
      " 2      293\n",
      "Name: dx_fold, dtype: int64\n",
      "read data from /home/ghhur/data/input/mimic_12_all_150_2020.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=2020 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cohort) 18536\n",
      "sampling ratio 0.3\n",
      "____readmission____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    10379\n",
      " 0     3708\n",
      " 1     3559\n",
      " 2      890\n",
      "Name: readmission_fold, dtype: int64\n",
      "____mortality____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    10379\n",
      " 0     3708\n",
      " 1     3559\n",
      " 2      890\n",
      "Name: mortality_fold, dtype: int64\n",
      "____los>3day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    10379\n",
      " 0     3708\n",
      " 1     3559\n",
      " 2      890\n",
      "Name: los>3day_fold, dtype: int64\n",
      "____los>7day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    10379\n",
      " 0     3708\n",
      " 1     3559\n",
      " 2      890\n",
      "Name: los>7day_fold, dtype: int64\n",
      "___diagnosis multi_label_stratified_split____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b809d4541ed5>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_sampled['dx_fold'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_split_results!!!!! \n",
      " -1    10360\n",
      " 0     3708\n",
      " 1     3565\n",
      " 2      903\n",
      "Name: dx_fold, dtype: int64\n",
      "read data from /home/ghhur/data/input/mimic_12_all_150_2020.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=2020 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cohort) 18536\n",
      "sampling ratio 0.5\n",
      "____readmission____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    7414\n",
      " 1    5931\n",
      " 0    3708\n",
      " 2    1483\n",
      "Name: readmission_fold, dtype: int64\n",
      "____mortality____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    7414\n",
      " 1    5931\n",
      " 0    3708\n",
      " 2    1483\n",
      "Name: mortality_fold, dtype: int64\n",
      "____los>3day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      " -1    7414\n",
      " 1    5931\n",
      " 0    3708\n",
      " 2    1483\n",
      "Name: los>3day_fold, dtype: int64\n",
      "____los>7day____ train and test split start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_split_results!!!!! \n",
      " -1    7414\n",
      " 1    5931\n",
      " 0    3708\n",
      " 2    1483\n",
      "Name: los>7day_fold, dtype: int64\n",
      "___diagnosis multi_label_stratified_split____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b809d4541ed5>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_sampled['dx_fold'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_split_results!!!!! \n",
      " -1    7427\n",
      " 1    5915\n",
      " 0    3708\n",
      " 2    1486\n",
      "Name: dx_fold, dtype: int64\n",
      "read data from /home/ghhur/data/input/mimic_12_all_150_2020.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=2020 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cohort) 18536\n",
      "sampling ratio 0.7\n",
      "____readmission____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      "  1    8304\n",
      "-1    4448\n",
      " 0    3708\n",
      " 2    2076\n",
      "Name: readmission_fold, dtype: int64\n",
      "____mortality____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      "  1    8304\n",
      "-1    4448\n",
      " 0    3708\n",
      " 2    2076\n",
      "Name: mortality_fold, dtype: int64\n",
      "____los>3day____ train and test split start!\n",
      "fold_split_results!!!!! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1    8304\n",
      "-1    4448\n",
      " 0    3708\n",
      " 2    2076\n",
      "Name: los>3day_fold, dtype: int64\n",
      "____los>7day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      "  1    8304\n",
      "-1    4448\n",
      " 0    3708\n",
      " 2    2076\n",
      "Name: los>7day_fold, dtype: int64\n",
      "___diagnosis multi_label_stratified_split____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b809d4541ed5>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_sampled['dx_fold'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_split_results!!!!! \n",
      "  1    8288\n",
      "-1    4469\n",
      " 0    3708\n",
      " 2    2071\n",
      "Name: dx_fold, dtype: int64\n",
      "read data from /home/ghhur/data/input/mimic_12_all_150_2020.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=2020 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/home/ghhur/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cohort) 18536\n",
      "sampling ratio 0.9\n",
      "____readmission____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      "  1    10676\n",
      " 0     3708\n",
      " 2     2670\n",
      "-1     1482\n",
      "Name: readmission_fold, dtype: int64\n",
      "____mortality____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      "  1    10676\n",
      " 0     3708\n",
      " 2     2670\n",
      "-1     1482\n",
      "Name: mortality_fold, dtype: int64\n",
      "____los>3day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      "  1    10676\n",
      " 0     3708\n",
      " 2     2670\n",
      "-1     1482\n",
      "Name: los>3day_fold, dtype: int64\n",
      "____los>7day____ train and test split start!\n",
      "fold_split_results!!!!! \n",
      "  1    10676\n",
      " 0     3708\n",
      " 2     2670\n",
      "-1     1482\n",
      "Name: los>7day_fold, dtype: int64\n",
      "___diagnosis multi_label_stratified_split____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-b809d4541ed5>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[fold_column] = -1\n",
      "<ipython-input-52-b809d4541ed5>:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_sampled['dx_fold'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_split_results!!!!! \n",
      "  1    10675\n",
      " 0     3708\n",
      " 2     2650\n",
      "-1     1503\n",
      "Name: dx_fold, dtype: int64\n",
      "seed_2021\n",
      "read data from /home/ghhur/data/input/mimic_12_all_150_2021.pkl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ghhur/data/input/mimic_12_all_150_2021.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3a12c5542183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0msampling_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_fold_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read data from'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}_{}_{}_{}_{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mcohort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}_{}_{}_{}_{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mcohort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx_one_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcohort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mcohort_fold_split\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mFewshot_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcohort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_valid_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ghhur/data/input/mimic_12_all_150_2021.pkl'"
     ]
    }
   ],
   "source": [
    "for src in source_list:\n",
    "    print('src_{}_start!'.format(src))\n",
    "    for item in items:\n",
    "        for random_state in random_state_list:\n",
    "            print('seed_{}'.format(random_state))\n",
    "            for i, sampled_ratio in enumerate(sampled_ratio_list):\n",
    "                sampling_fold = sampling_fold_list[i]\n",
    "                print('read data from', os.path.join(input_dir, '{}_{}_{}_{}_{}.pkl'.format(src,time_window,item,max_len,random_state)))\n",
    "                cohort = pd.read_pickle(os.path.join(input_dir, '{}_{}_{}_{}_{}.pkl'.format(src,time_window,item,max_len,random_state))).reset_index(drop=True)\n",
    "                cohort, columns = dx_one_hot_encoding(cohort)\n",
    "                cohort_fold_split =Fewshot_sampling(target_cols, cohort, random_state, sampled_ratio, sampling_fold, train_valid_fold, columns)\n",
    "                cohort_fold_split= cohort_fold_split.drop(columns=columns)\n",
    "                #save_file = '{}_{}_{}_{}_{}_{}_concat.pkl'.format(src,time_window,item, max_len,random_state, int(sampled_ratio*100))\n",
    "                save_file = '{}_{}_{}_{}_{}_{}.pkl'.format(src,time_window,item, max_len,random_state, int(sampled_ratio*100))\n",
    "                cohort_fold_split.to_pickle(os.path.join(input_dir, save_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi processing for split and few shot sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 'all'\n",
    "random_state_list = [2020,2021,2022,2023,2024,2025,2026,2027,2028,2029]\n",
    "def multi_for_split(random_state):      \n",
    "    for src in source_list:\n",
    "        cohort = pd.read_pickle(os.path.join(input_dir, '{}_12_all_150.pkl'.format(src))).reset_index(drop=True)\n",
    "        cohort, columns = dx_one_hot_encoding(cohort)\n",
    "        cohort_fold_split =train_valid_test_split(target_cols, cohort, random_state, test_ratio, train_valid_fold, columns)\n",
    "        cohort_fold_split= cohort_fold_split.drop(columns=columns)\n",
    "        save_file = '{}_12_all_150_{}.pkl'.format(src, random_state)\n",
    "        print('save to .. ', os.path.join(input_dir, save_file))\n",
    "        cohort_fold_split.to_pickle(os.path.join(input_dir, save_file))\n",
    "        \n",
    "def multi_for_fewshot(random_state):\n",
    "    for src in source_list:\n",
    "        print('src_{}_start!'.format(src))\n",
    "        for i, sampled_ratio in enumerate(sampled_ratio_list):\n",
    "            sampling_fold = sampling_fold_list[i]\n",
    "            cohort = pd.read_pickle(os.path.join(input_dir, '{}_{}_{}_{}_{}.pkl'.format(src,time_window,item,max_len,random_state))).reset_index(drop=True)\n",
    "            cohort, columns = dx_one_hot_encoding(cohort)\n",
    "            cohort_fold_split =Fewshot_sampling(target_cols, cohort, random_state, sampled_ratio, sampling_fold, train_valid_fold, columns)\n",
    "            cohort_fold_split= cohort_fold_split.drop(columns=columns)\n",
    "            #save_file = '{}_{}_{}_{}_{}_{}_concat.pkl'.format(src,time_window,item, max_len,random_state, int(sampled_ratio*100))\n",
    "            save_file = '{}_{}_{}_{}_{}_{}.pkl'.format(src,time_window,item, max_len,random_state, int(sampled_ratio*100))\n",
    "            cohort_fold_split.to_pickle(os.path.join(input_dir, save_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 128 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:  1.3min remaining:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:  1.3min remaining:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed:  1.3min remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  1.3min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  1.3min remaining:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  1.3min remaining:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:  1.3min remaining:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "multi_process = Parallel(n_jobs=-1, verbose=100)(delayed(multi_for_split)(random_state) for random_state in random_state_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 128 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:  4.4min remaining: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:  4.5min remaining: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed:  4.5min remaining:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  4.6min remaining:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:  4.7min remaining:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  4.7min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:  4.7min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.8min finished\n"
     ]
    }
   ],
   "source": [
    "multi_process = Parallel(n_jobs=-1, verbose=100)(delayed(multi_for_fewshot)(random_state) for random_state in random_state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
